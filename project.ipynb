{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Data Preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              tweet1  \\\n",
      "0             #CouchSelfiepic.twitter.com/AZDIB88OwR   \n",
      "1  New GOP platform now includes language that su...   \n",
      "2  Glad to contribute to the Tesla museum and wil...   \n",
      "3  #PrayForSyria watch this and think like a chil...   \n",
      "4  NY! #25FOR1 Auction tonight at @No8NY with all...   \n",
      "\n",
      "                                              tweet2  similarity_label  \n",
      "0             #CouchSelfiepic.twitter.com/AZDIB88OwR                 1  \n",
      "1  President Obama on Romney: \"We know what chang...                 0  \n",
      "2  Glad to contribute to the Tesla museum and wil...                 1  \n",
      "3  tweeted July 30 2011 - \"what makes you so beau...                 0  \n",
      "4  Keep sending them to weare50million@gmail.com ...                 0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Charger les données à partir des fichiers CSV\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "# Créer une liste de tous les utilisateurs uniques\n",
    "unique_users = train_df['user'].unique()\n",
    "\n",
    "# Initialiser la liste pour stocker les paires de tweets\n",
    "tweet_pairs = []\n",
    "\n",
    "# Déterminer le nombre maximum de paires à générer pour chaque utilisateur\n",
    "max_pairs_per_user = len(train_df) // len(unique_users)\n",
    "\n",
    "# Boucler à travers chaque utilisateur\n",
    "for user in unique_users:\n",
    "    # Extraire tous les tweets de l'utilisateur actuel\n",
    "    user_tweets = train_df[train_df['user'] == user]['text'].tolist()\n",
    "    \n",
    "    # Générer des paires de tweets à partir du même utilisateur\n",
    "    same_user_pairs = random.sample(list(zip(user_tweets, user_tweets)), max_pairs_per_user)\n",
    "    \n",
    "    # Extraire les tweets des autres utilisateurs\n",
    "    other_users_tweets = train_df[train_df['user'] != user]['text'].tolist()\n",
    "    \n",
    "    # Sélectionner aléatoirement un nombre équivalent de tweets des autres utilisateurs\n",
    "    random_other_tweets = random.sample(other_users_tweets, max_pairs_per_user)\n",
    "    \n",
    "    # Créer des paires de tweets avec des utilisateurs différents\n",
    "    different_user_pairs = list(zip(user_tweets, random_other_tweets))\n",
    "    \n",
    "    # Ajouter les paires de tweets à la liste avec une étiquette de similarité 1 pour les mêmes utilisateurs\n",
    "    tweet_pairs.extend([[pair[0], pair[1], 1] for pair in same_user_pairs])\n",
    "    \n",
    "    # Ajouter les paires de tweets à la liste avec une étiquette de similarité 0 pour les différents utilisateurs\n",
    "    tweet_pairs.extend([[pair[0], pair[1], 0] for pair in different_user_pairs])\n",
    "\n",
    "# Mélanger les paires de tweets pour assurer un ordre aléatoire\n",
    "random.shuffle(tweet_pairs)\n",
    "\n",
    "# Convertir en DataFrame\n",
    "tweet_pairs_df = pd.DataFrame(tweet_pairs, columns=['tweet1', 'tweet2', 'similarity_label'])\n",
    "\n",
    "# Afficher un aperçu du DataFrame\n",
    "print(tweet_pairs_df.head())\n",
    "\n",
    "# Enregistrer le DataFrame dans un fichier CSV\n",
    "tweet_pairs_df.to_csv('balanced_tweet_pairs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\khadidja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\khadidja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\khadidja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              tweet1  \\\n",
      "0            # couchselfiepic twitter com azdib88owr   \n",
      "1  new gop platform includes language support bor...   \n",
      "2  glad contribute tesla museum future great man ...   \n",
      "3  # prayforsyria watch think like child love ans...   \n",
      "4  ny # 25for1 auction tonight @ no8ny proceeds g...   \n",
      "\n",
      "                                              tweet2  similarity_label  \n",
      "0            # couchselfiepic twitter com azdib88owr                 1  \n",
      "1  president obama romney know change look like s...                 0  \n",
      "2  glad contribute tesla museum future great man ...                 1  \n",
      "3  tweeted july 30 2011 make beautiful dont know ...                 0  \n",
      "4  keep sending weare50million @ gmail com soon s...                 0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Télécharger les ressources nécessaires pour NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Charger le DataFrame\n",
    "df = pd.read_csv('balanced_tweet_pairs.csv')\n",
    "\n",
    "# Initialiser le lemmatiseur\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Fonction de nettoyage du texte avec lemmatisation\n",
    "def clean_text(text):\n",
    "    # Convertir en minuscules\n",
    "    text = text.lower()\n",
    "    # Supprimer la ponctuation, les symboles et les autres caractères spéciaux sauf les hashtags et les mentions\n",
    "    text = ''.join([char if char in ['#', '@'] or char not in string.punctuation else ' ' for char in text])\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Supprimer les mots vides (stop words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatisation\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Rejoindre les tokens pour former une chaîne de texte\n",
    "    cleaned_text = ' '.join(lemmatized_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "# Appliquer la fonction de nettoyage du texte à chaque tweet dans le DataFrame\n",
    "df['tweet1'] = df['tweet1'].apply(clean_text)\n",
    "df['tweet2'] = df['tweet2'].apply(clean_text)\n",
    "\n",
    "# Afficher un aperçu du DataFrame après le prétraitement\n",
    "print(df.head())\n",
    "\n",
    "# Enregistrer le DataFrame prétraité dans un nouveau fichier CSV\n",
    "df.to_csv('preprocessed_balanced_tweet_pairs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Model Architecture and Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1170/1170 [==============================] - 5s 3ms/step - loss: 13.5993 - accuracy: 0.0211 - val_loss: 0.7499 - val_accuracy: 0.4945\n",
      "Epoch 2/10\n",
      "1170/1170 [==============================] - 4s 3ms/step - loss: 0.4390 - accuracy: 0.6752 - val_loss: 0.2828 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "1170/1170 [==============================] - 4s 3ms/step - loss: 0.2021 - accuracy: 0.9999 - val_loss: 0.1426 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "1170/1170 [==============================] - 4s 3ms/step - loss: 0.1061 - accuracy: 0.9999 - val_loss: 0.0779 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "1170/1170 [==============================] - 4s 3ms/step - loss: 0.0597 - accuracy: 0.9998 - val_loss: 0.0449 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1170/1170 [==============================] - 5s 4ms/step - loss: 0.0350 - accuracy: 0.9998 - val_loss: 0.0266 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "1170/1170 [==============================] - 4s 4ms/step - loss: 0.0211 - accuracy: 0.9998 - val_loss: 0.0161 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1170/1170 [==============================] - 4s 3ms/step - loss: 0.0130 - accuracy: 0.9998 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1170/1170 [==============================] - 4s 3ms/step - loss: 0.0081 - accuracy: 0.9998 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1170/1170 [==============================] - 4s 4ms/step - loss: 0.0052 - accuracy: 0.9998 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.0041 - accuracy: 0.9997\n",
      "Test Loss: 0.004054030869156122\n",
      "Test Accuracy: 0.9997115135192871\n",
      "325/325 [==============================] - 1s 3ms/step\n",
      "Precision: 0.999414976599064\n",
      "Recall: 1.0\n",
      "F1 Score: 0.9997074027114015\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Subtract, Dense, Lambda\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Chargement des données\n",
    "df = pd.read_csv('preprocessed_balanced_tweet_pairs.csv')\n",
    "\n",
    "\n",
    "# Fonction pour prétraiter les tweets\n",
    "def preprocess_tweet(tweet):\n",
    "    return tweet.split()\n",
    "\n",
    "# Remplacer les valeurs NaN par une chaîne vide\n",
    "df['tweet1'] = df['tweet1'].fillna('')\n",
    "df['tweet2'] = df['tweet2'].fillna('')\n",
    "\n",
    "# Prétraitement des tweets et création des listes de listes de mots\n",
    "df['tweet1'] = df['tweet1'].apply(preprocess_tweet)\n",
    "df['tweet2'] = df['tweet2'].apply(preprocess_tweet)\n",
    "\n",
    "# Entraînement du modèle Word2Vec \n",
    "embedding_dim = 100  # Taille de l'embedding\n",
    "word2vec_model = Word2Vec(sentences=df['tweet1'].tolist() + df['tweet2'].tolist(),\n",
    "                          vector_size=embedding_dim, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Fonction pour obtenir la représentation vectorielle d'un tweet à partir du modèle Word2Vec\n",
    "def get_tweet_embedding(tweet, model):\n",
    "    embedding = []\n",
    "    for word in tweet:\n",
    "        if word in model.wv:\n",
    "            embedding.append(model.wv[word])\n",
    "    if embedding:\n",
    "        return np.mean(embedding, axis=0)\n",
    "    else:\n",
    "        return np.zeros(embedding_dim)\n",
    "\n",
    "# Obtenir les représentations vectorielles des tweets\n",
    "df['tweet1_embedding'] = df['tweet1'].apply(lambda x: get_tweet_embedding(x, word2vec_model))\n",
    "df['tweet2_embedding'] = df['tweet2'].apply(lambda x: get_tweet_embedding(x, word2vec_model))\n",
    "\n",
    "# Fonction pour calculer la distance de Manhattan entre deux vecteurs\n",
    "def manhattan_distance(vec1, vec2):\n",
    "    return K.sum(K.abs(vec1 - vec2), axis=-1, keepdims=True)\n",
    "\n",
    "# Inputs\n",
    "input1 = Input(shape=(embedding_dim,))\n",
    "input2 = Input(shape=(embedding_dim,))\n",
    "\n",
    "# Manhattan Distance\n",
    "distance = Lambda(lambda tensors: manhattan_distance(tensors[0], tensors[1]))([input1, input2])\n",
    "\n",
    "# Dense Layer\n",
    "output = Dense(1, activation='sigmoid')(distance)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entraînement du modèle\n",
    "model.fit([np.array(df['tweet1_embedding'].tolist()), np.array(df['tweet2_embedding'].tolist())],\n",
    "          df['similarity_label'], epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Charger les données de test\n",
    "test_df = pd.read_csv('preprocessed_test_tweet_pairs.csv')\n",
    "\n",
    "# Prétraiter les tweets de l'ensemble de test \n",
    "test_df['tweet1'] = test_df['tweet1'].fillna('').apply(preprocess_tweet)\n",
    "test_df['tweet2'] = test_df['tweet2'].fillna('').apply(preprocess_tweet)\n",
    "\n",
    "# Obtenir les représentations vectorielles des tweets pour l'ensemble de test \n",
    "test_df['tweet1_embedding'] = test_df['tweet1'].apply(lambda x: get_tweet_embedding(x, word2vec_model))\n",
    "test_df['tweet2_embedding'] = test_df['tweet2'].apply(lambda x: get_tweet_embedding(x, word2vec_model))\n",
    "\n",
    "# Évaluation du modèle\n",
    "loss, accuracy = model.evaluate([np.array(test_df['tweet1_embedding'].tolist()), np.array(test_df['tweet2_embedding'].tolist())],\n",
    "                                test_df['similarity_label'])\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "predictions = model.predict([np.array(test_df['tweet1_embedding'].tolist()), np.array(test_df['tweet2_embedding'].tolist())])\n",
    "predictions = (predictions > 0.5).astype(int)\n",
    "\n",
    "\n",
    "# Labels réels\n",
    "true_labels = test_df['similarity_label'].values\n",
    "\n",
    "# Calcul de la précision\n",
    "precision = precision_score(true_labels, predictions)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calcul du rappel\n",
    "recall = recall_score(true_labels, predictions)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calcul du score F1\n",
    "f1 = f1_score(true_labels, predictions)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
