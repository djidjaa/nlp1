{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41d26a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              tweet1  \\\n",
      "0  @JustBe74 V important to make all of humanity ...   \n",
      "1  All North American dates on #THEPRISMATICWORLD...   \n",
      "2  Here's an in-depth look at how our economy has...   \n",
      "3  The @Bucks run out to the court as they look t...   \n",
      "4  CAN'T WAIT to watch @kathybethterry's online c...   \n",
      "\n",
      "                                              tweet2  similarity_label  \n",
      "0  Crooked Hillary no longer has credibility - to...                 0  \n",
      "1  All North American dates on #THEPRISMATICWORLD...                 1  \n",
      "2  Pixar’s new movie will hit you right in the fe...                 0  \n",
      "3  Leonardo Ulloa seals Brighton return\\n\\nMore o...                 0  \n",
      "4  CAN'T WAIT to watch @kathybethterry's online c...                 1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Charger les données à partir des fichiers CSV\n",
    "train_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Créer une liste de tous les utilisateurs uniques\n",
    "unique_users = train_df['user'].unique()\n",
    "\n",
    "# Initialiser la liste pour stocker les paires de tweets\n",
    "tweet_pairs = []\n",
    "\n",
    "# Déterminer le nombre maximum de paires à générer pour chaque utilisateur\n",
    "max_pairs_per_user = len(train_df) // len(unique_users)\n",
    "\n",
    "# Boucler à travers chaque utilisateur\n",
    "for user in unique_users:\n",
    "    # Extraire tous les tweets de l'utilisateur actuel\n",
    "    user_tweets = train_df[train_df['user'] == user]['text'].tolist()\n",
    "    \n",
    "    # Générer des paires de tweets à partir du même utilisateur\n",
    "    same_user_pairs = random.sample(list(zip(user_tweets, user_tweets)), max_pairs_per_user)\n",
    "    \n",
    "    # Extraire les tweets des autres utilisateurs\n",
    "    other_users_tweets = train_df[train_df['user'] != user]['text'].tolist()\n",
    "    \n",
    "    # Sélectionner aléatoirement un nombre équivalent de tweets des autres utilisateurs\n",
    "    random_other_tweets = random.sample(other_users_tweets, max_pairs_per_user)\n",
    "    \n",
    "    # Créer des paires de tweets avec des utilisateurs différents\n",
    "    different_user_pairs = list(zip(user_tweets, random_other_tweets))\n",
    "    \n",
    "    # Ajouter les paires de tweets à la liste avec une étiquette de similarité 1 pour les mêmes utilisateurs\n",
    "    tweet_pairs.extend([[pair[0], pair[1], 1] for pair in same_user_pairs])\n",
    "    \n",
    "    # Ajouter les paires de tweets à la liste avec une étiquette de similarité 0 pour les différents utilisateurs\n",
    "    tweet_pairs.extend([[pair[0], pair[1], 0] for pair in different_user_pairs])\n",
    "\n",
    "# Mélanger les paires de tweets pour assurer un ordre aléatoire\n",
    "random.shuffle(tweet_pairs)\n",
    "\n",
    "# Convertir en DataFrame\n",
    "tweet_pairs_df = pd.DataFrame(tweet_pairs, columns=['tweet1', 'tweet2', 'similarity_label'])\n",
    "\n",
    "# Afficher un aperçu du DataFrame\n",
    "print(tweet_pairs_df.head())\n",
    "\n",
    "# Enregistrer le DataFrame dans un fichier CSV\n",
    "tweet_pairs_df.to_csv('test_tweet_pairs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0011e625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\khadidja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\khadidja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\khadidja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              tweet1  \\\n",
      "0  @ justbe74 v important make humanity proud cas...   \n",
      "1  north american date # theprismaticworldtour sa...   \n",
      "2  depth look economy made real progress last six...   \n",
      "3  @ buck run court look get 11th win 14 game str...   \n",
      "4  wait watch @ kathybethterry online chat @ www ...   \n",
      "\n",
      "                                              tweet2  similarity_label  \n",
      "0  crooked hillary longer credibility much failur...                 0  \n",
      "1  north american date # theprismaticworldtour sa...                 1  \n",
      "2  pixar ’ new movie hit right feel http goo gl v...                 0  \n",
      "3  leonardo ulloa seal brighton return loan deal ...                 0  \n",
      "4  wait watch @ kathybethterry online chat @ www ...                 1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Télécharger les ressources nécessaires pour NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Charger le DataFrame\n",
    "df = pd.read_csv('test_tweet_pairs.csv')\n",
    "\n",
    "# Initialiser le lemmatiseur\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Fonction de nettoyage du texte avec lemmatisation\n",
    "def clean_text(text):\n",
    "    # Convertir en minuscules\n",
    "    text = text.lower()\n",
    "    # Supprimer la ponctuation, les symboles et les autres caractères spéciaux sauf les hashtags et les mentions\n",
    "    text = ''.join([char if char in ['#', '@'] or char not in string.punctuation else ' ' for char in text])\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Supprimer les mots vides (stop words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatisation\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Rejoindre les tokens pour former une chaîne de texte\n",
    "    cleaned_text = ' '.join(lemmatized_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "# Appliquer la fonction de nettoyage du texte à chaque tweet dans le DataFrame\n",
    "df['tweet1'] = df['tweet1'].apply(clean_text)\n",
    "df['tweet2'] = df['tweet2'].apply(clean_text)\n",
    "\n",
    "# Afficher un aperçu du DataFrame après le prétraitement\n",
    "print(df.head())\n",
    "\n",
    "# Enregistrer le DataFrame prétraité dans un nouveau fichier CSV\n",
    "df.to_csv('preprocessed_test_tweet_pairs.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718785b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
